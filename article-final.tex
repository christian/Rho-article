\documentclass[11pt]{amsart}
\usepackage{studia}
\usepackage{booktabs}
\usepackage[numbers]{natbib}

% This is now the recommended way for checking for PDFLaTeX:
\usepackage{ifpdf}
\ifpdf
\usepackage[pdftex]{graphicx}
\else
\usepackage{graphicx}
\fi

\setcounter{page}{1} % to be added afterwards
\coordinates{LV}{3}{2010}

\title{Design of a Recommender System using Collaborative Filtering}
\author{Prodan Andrei-Cristian}
% \address{...}
\email{prodan.cristian@gmail.com}
\date{30.08.2010} % submission date 

% AMS Subject Classification (2010)
\subjclass[2010]{68P20, 15A18, 15A23} 

% ACM Computing Classification System (1998)
\subjclassCR{%
code [\textbf{Information Systems}]: Subtopic -- \textit{Information storage and retrieval};
code [\textbf{Mathematics of Computing}]: Subtopic -- \textit{Numerical Analysis}}

\keywords{collaborative filtering, recommender systems, svd}

\begin{document}

\begin{abstract}
Nowadays, consumers have a lot of choices. Electronic retailers offer a great variety of products. Because of this, there is a need for Recommender Systems. These systems aim to solve the problem of matching consumers with the most appealing products for them. They do this by analyzing either the products information details (Content Based methods) or users social behavior (Collaborative Filtering). This article starts with an overview on these two approaches used to build Recommender Systems with a special emphasis on Collaborative Filtering (CF). It then presents on of the best methods for CF: the Matrix Factorization technique. Next, it presents two algorithms used for matrix factorization. A framework created by the author, called Rho, that uses these techniques is then presented. In the end, we present some results obtained after some experiments ran using this framework.
\end{abstract}

\maketitle

\section{Introduction}

Nowadays, consumers have a lot of choices. Electronic retailers offer a great variety of products. Because of this, there is a need for Recommender Systems. These systems aim to solve the problem of matching consumers with the most appropriate products. 

As an example, they can be used on products such as books, movies, music, restaurants and TV shows. Many customers will view the same movie or purchase the same item. Every item has a couple of characteristics like genre or subject, and users can express their preferences (like of dislike) regarding them. Also, customers give feedback on products, indicating how much they liked it, so data about which product appeals to which customer, is available. Companies can analyze this data and recommend products to their customers.

Essentially, Recommender Systems compare the user's profile to some reference characteristics, and try to predict the ``rating'' that a user would give to an item they had not yet considered.

These characteristics may be from the information of the item (the \emph{content-based approach}, section \ref{content_based_methods}) or the user's social environment (the \emph{collaborative filtering} approach, section \ref{sec:cf}).

There are mainly two forms of data collection needed for building a users profile:
\begin{enumerate}
  \item \textbf{Explicit data}: ask a user to rate an item on a scale, present two items to a user and asking him/her to choose the best one, ask a user to create a list of items that he/she likes, ask a user to rank a collection of items from favorite to least favorite;
  \item \textbf{Implicit data}: observing the items that a user views in an online store, analyze item/user viewing times, keep a record of the items that a user purchases online, obtaining a list of items that a user has listened to or watched on his/her computer, analyzing the user's social network and discovering similar likes and dislikes.
\end{enumerate}

The recommender system compares the collected data to similar and non similar data collected from others and calculates a list of recommended items for the user.

Big companies like Amazon, Google, Yahoo, Netflix, last.fm also use Recommender Systems.

\subsection{Challenges in building Recommender systems}
 The following three issues represent the main challenges one has to think about when creating a Recommender System.
 
\begin{itemize}
  \item The cold-start problem. Recommender systems must be capable of matching the characteristics of an item against relevant features in the user's profile. In order to do this, it must first construct a sufficiently-detailed model of the user's tastes and preferences through preference elicitation. This may be done either explicitly (by querying the user) or implicitly (by observing the user's behavior). In both cases, the cold start problem would imply that the user has to dedicate an amount of effort to contribute to the construction of their user profile – before the system can start providing any intelligent recommendations.
  In the collaborative filtering approach, users have to provide ratings, in order for the system to start and make recommendations.
  \item Sparsity. In any recommender system, the number of ratings already obtained is usually very small compared to the number of ratings that need to be predicted. Effective prediction of ratings from a small number of examples is important. Also, the success of the collaborative recommender system depends on the availability of a critical mass of users. For example, in the movie recommendation system there may be many movies that have been rated only by few people and these movies would be recommended very rarely, even if those few users gave high ratings to them.
  \item Scalability. Recommender systems are usually designed to work on very large data sets. Therefore the scalability of the methods employed by them systems is crucial. CF systems often have to manage millions of users or items.
\end{itemize}
 

\subsection{Content-Based methods}
\label{content_based_methods}
\emph{Content-Based recommender systems} are systems that recommend an item to an user, based upon a description of the item and a profile of the user's interests. Content-based recommendation systems may be used in a variety of domains ranging from recommending web pages, news articles, restaurants, television programs, and items for sale.

A variety of machine learning algorithms have been adapted to learning user profiles, and the choice of learning algorithm depends upon the representation of content. They are presented with details in \cite{content_based_reco_syst}.

\subsection{Collaborative Filtering}
\label{sec:cf}

\emph{Collaborative filtering} is a term coined by the developers of Tapestry, the first recommender system (\cite{Goldberg92usingcollaborative}). The underlying assumption of CF approach is that those who agreed in the past tend to agree again in the future. For example, a collaborative filtering or recommendation system for music tastes could make predictions about which music a user should like given a partial list of that user's tastes (likes or dislikes). These predictions are specific to the user, but use information gathered from many users. 

Researchers have devised a number number of collaborative filtering algorithms that can be divided into two main categories 
\cite{Sarwar01item-basedcollaborative}: \emph{Memory-based (user based)} and \emph{Model-based (item-based)} algorithms. In this article, we will describe the model-based algorithms.

The problem of collaborative filtering (CF) is defined in \cite{takacs09scalable}, as follows. The problem can be modeled by the random triplet $(U,I,R)$, where:
  \begin{itemize}
    \item $U$ taking values from $\{1,...,N\}$ is the user identifier ($N$ is the number of users),
    \item $I$ taking values from $\{1,...,M\}$ is the item identifier ($M$ is the number of items), and
    \item $R$ taking values from $X \subset R$ is the rating value. Typical rating values can be binary ($X = \{0,1\}$), integers from a given range (for example, $X = \{1,2,3,4,5\}$), or real numbers of a closed interval (for example, $X = [−10, 10]$).
  \end{itemize}

A realization of $(U,I,R)$ denoted by $(u,i,r)$ means that user $u$ rated item $i$ with value $r$. The goal is to estimate $R$ from $(U,I)$ such that the \emph{root mean squared error} of the estimate,

\begin{equation}
  \label{rmse_1}
  RMSE = \sqrt{E\{(\hat{R}-R)^2\}}
\end{equation}

is minimal, where $\hat{R}$ is the square estimate of $R$.

In practice, the distribution of $(U,I,R)$ is not known: we are only given a finite sample, $\mathcal{T}' = \{(u_1,i_1,r_1),(u_2,i_2,r_2),...,(u_t,i_t,r_t)\}$, generated by it. The sample $\mathcal{T}'$ can be used for training predictors. We assume sampling without replacement in the sense that $(user ID, item ID)$ pairs are unique in the sample, which means that users do not rate items more than once. Let us introduce the notation $\mathcal{T} =\{(u,i):\exists{r}:(u,i,r) \in \mathcal{T}' \}$ for the set of $(userID, itemID)$ pairs. Note that $|\mathcal{T}'|=|T|$, and typically $|\mathcal{T}| \ll N \cdot M$, because most of the users rate only a small subset of the entire set of items. The sample can be represented as a partially specified matrix denoted by $\mathbf{R} \in \mathbb{R}^{N \times M}$, where the matrix elements are known in positions $(u,i) \in \mathcal{T}$ , and unknown in positions $(u,i) \notin T$ . The value of the matrix $\mathbf{R}$ at position $(u,i) \in \mathcal{T}$ , denoted by $r_{ui}$, stores the rating of user $u$ for item $i$. For clarity, we use the term $(u,i)$-th rating in general for $r_ui$, and $(u,i)$-th training example if $r_{ui} : (u,i) \in \mathcal{T}$.

The goal of this CF setup is to create such predictors that aim at minimizing the error (\ref{rmse_1}). In practice, we cannot measure the error because the distribution of $(U,I,R)$ is unknown, but we can estimate the error on a validation set. Let us denote the validation set by $\mathcal{V}' \subset [1,...,N] \times [1,...,M] \times \mathcal{X}$, assuming sampling without replacement as defined above, and we further assume the uniqueness of $(userID,itemID)$ pairs across $\mathcal{T}'$ and $\mathcal{V}'$. We define $\mathcal{V} = \{(u,i):\exists{r}:(u,i,r) \in \mathcal{V'}\}$. The assumptions ensure that $\mathcal{T} \cap \mathcal{V} = \emptyset$ . If both the training set $\mathcal{T}$ and validation set $\mathcal{V}'$ are generated from the same distribution the estimate of RMSE can be calculated as:

\begin{equation}
  \label{rmse_2}
  \hat{RMSE} = \sqrt{\frac{1}{|\mathcal{V}|} \sum_{(u,i)\in\mathcal{V}} (\hat{r_{ui}} - r_{ui})^2}
\end{equation}

\section{Matrix Factorization for Collaborative Filtering}

The idea behind MF is quite simple. We want to approximate matrix $R$ (the ratings matrix) as the product of two matrices:

\begin{equation}
  \mathbf{R} \approx \mathbf{P}\mathbf{Q},
\end{equation}

where $\mathbf{P}$ is an $N \times K$ matrix and $\mathbf{Q}$ is a $K \times M$ matrix. We call $P$ the user feature matrix and $Q$ the item feature matrix. $K$ is the number of features in the given factorization. $\mathbf{Q}$ and $\mathbf{P}$ typically contain real numbers, even when R contains only integers.

One way to do this is to use SVD. 

SVD is a matrix factorization technique commonly used for producing \emph{low-rank} approximations. Given an $m \times n$ matrix $A$, with rank $r$, the singular value decomposition, $SVD(A)$, is defined as:

\begin{equation}
  SVA(A) = U \times S \times V^T
\end{equation}

Getting back to collaborative filtering, the task is to factorize the $R$ (rating matrix) according to SVD. Once the $m \times n$ ratings matrix $R$ is decomposed and reduced into three SVD component matrices with $k$ features $U_k$, $S_k$, and $V_k$, prediction can be generated from it by computing the cosine similarities (dot products) between $m$ pseudo-customers $U_k \cdot {\sqrt{S_k}^T}$ and $n$ pseudo-products ${\sqrt{S_k} \cdot {V_k}^T}$. In particular, the prediction score $P_{i,j}$ for the $i$-th customer on the $j$-th product by adding the row average $r_i$ to the similarity. Formally, 
\begin{equation}
  P_{i,j} = r + U_k \cdot {\sqrt{S_k}^T}(i) \cdot {\sqrt{S_k} \cdot {V_k}^T}
\end{equation}
Once the SVD decomposition is done, the prediction generation process involves only a dot product computation, which takes $O(1)$ time, since $k$ is a constant.

However, this is unfeasible for very big and sparse matrices. An alternative to this is proposed by \cite{takacs09scalable} and presented next. 

\subsection{Background on the ISMF and RISMF algorithms}
\label{ismf}

In this section we give an overview of the theoretical aspects of two recommendation algorithms used in the framework implemented by the author and described in section \ref{sec:rho}. These algorithms (called be thier authors ISMF and RISMF \cite{takacs09scalable}) use a matrix factorization technique.

The notations are the same used in section \ref{sec:cf}. Let $p_{uk}$ denote the elements of $\mathbf{P} \in \mathbb{R}^{N \times K}$, and $q_{ki}$ the elements of $Q \in \mathbb{R}^{K \times M}$. Further, let $\mathbf{p_u}$, denote a row (vector) of $\mathbf{P}$, and $\mathbf{q_i}$, a column (vector) of $\mathbf{Q}$. Then:

\begin{eqnarray}
 \hat{r_{ui}}   &=& \sum_{k=1}^K p_{uk}q_{ki} = \mathbf{p_u}\mathbf{q_i}, \\
       e_{ui}   &=& r_{ui}-\hat{r_{ui}}, (u, i) = r_{ui} - {\mathbf{p_u}\mathbf{q_i}} \in \mathcal(T), \nonumber \\
       {e_{ui}}'&=& \frac{1}{2}{e_{ui}}^2, \\
       SSE      &=& \sum_{(u,i)\in \mathcal{T}} {e_{ui}}^2 = \sum_{(u,i)\in \mathcal{T}} {\left( r_{ui} - \sum_{k=1}^K p_{uk}q_{ki} \right)}^2 \nonumber \\
       SSE'     &=& \frac{1}{2}SSE = \sum_{(u,i)\in \mathcal{T}} {e_{ui}}', \nonumber \\
       RMSE     &=& \sqrt{\frac{SSE}{|\mathcal{T}|}}, \nonumber \\ 
       (\mathbf{P*}, \mathbf{Q*}) &=& \arg\!\min_{\boldsymbol{(\mathbf{P*}, \mathbf{Q*})}}SSE'  = \arg\!\min_{\boldsymbol{(\mathbf{P*}, \mathbf{Q*})}} SSE = \arg\!\min_{\boldsymbol{(\mathbf{P*}, \mathbf{Q*})}} RMSE \label{eq4}
\end{eqnarray}

Here:

\begin{itemize}
	\item $\hat{r_{ui}}$ denotes how the $u$-th user would rate the $i$-th item, according to the model;
	\item $e_{ui}$ denotes the training error measured at the $(u,i)$-th rating;
	\item $SSE$ denotes the sum of squared training errors.
\end{itemize}

Equation (3.4) states that the optimal $\mathbf{P}$ and $\mathbf{Q}$ minimize the sum of squared errors only on the known elements of $\mathbf{R}$

In order to minimize $RMSE$, which is in this case equivalent to minimizing $SSE'$, we apply a simple incremental gradient descent method to find a local minimum of $SSE'$, where one gradient step intends to decrease the square of prediction error of only one rating, or equivalently, either ${e_{ui}}'$ or ${e_{ui}}^2$.

For the incremental gradient descent method, suppose we are at the $(u,i)$-th training example, $r_{ui}$, and its approximation $\hat{r_{ui}}$ is given.

We compute the gradient of ${e_{ui}}'$ and we obtain:

\begin{eqnarray}
  \nabla{{e_{ui}}'} &=& \left( \frac{\partial{e_{ui}}'}{\partial{\mathbf{p_u}}}, 
                               \frac{\partial{e_{ui}}'}{\partial{\mathbf{q_i}}} \right) \\
 \frac{\partial{e_{ui}}'}{\partial{\mathbf{p_{uk}}}} &=& -e_{ui} \cdot \mathbf{q_{ki}} \\ 
 \frac{\partial{e_{ui}}'}{\partial{\mathbf{q_{ki}}}} &=& -e_{ui} \cdot \mathbf{p_{uk}} \\
\end{eqnarray}

We update the weights in the direction opposite to the gradient:

\begin{eqnarray}
  p_{uk} &\leftarrow& p_{uk} + \gamma \cdot e_{ui} \cdot q_{ki} \\
  q_{ki} &\leftarrow& q_{ki} + \gamma \cdot e_{ui} \cdot p_{uk} 
\end{eqnarray}

That is, we change the weights in $\mathbf{P}$ and $\mathbf{Q}$ to decrease the square of actual error, thus better approximating $r_{ui}$. Here $\gamma$ is the learning rate. 

When the training has been finished, each value of $\mathbf{R}$ can be computed easily using Eq. , even at unknown positions. In other words, the model (\textbf{P*}, \textbf{Q*}) provides a description of how an arbitrary user would rate any item.

This method is called \textbf{ISMF}, that is incremental simultaneous MF, according to \cite{takacs09scalable} due to its distinctive incremental and simultaneous weight updating to other MF methods.

\subsection{Improving the ISMF algorithm}
\label{rismf}

The matrix factorization presented in the previous section can overfit for users with few (no more than $K$) ratings: assuming that the feature vectors of the items rated by the user are linearly independent and $\mathbf{Q}$ does not change, there exists a user feature vector with zero training error. Thus, there is a potential for overfitting, if $\gamma$ and the extent of the change in $Q$ are both small. A common way to avoid overfitting is to apply regularization by penalizing the square of the Euclidean norm of weights.
Penalizing the weights results in a new optimization problem:

\begin{eqnarray}
  {e_{ui}}' &=& \frac{{e_{ui}}^2 + \lambda \cdot \mathbf{p_u} \cdot \mathbf{p_u}^T + \lambda \cdot \mathbf{q_i}^T \cdot \mathbf{q_i}}{2}, \nonumber \\
  SSE' &=& \sum_{(u, i) \in \mathcal{T}} {e_{ui}}', \nonumber \\
  (\mathbf{P*}, \mathbf{Q*}) &=& \arg\!\min_{\boldsymbol{(\mathbf{P}, \mathbf{Q})}}SSE'.
\end{eqnarray}

Here $\lambda \ge 0$ is the regularization factor. Note that minimizing $SSE''$ is no longer equivalent to minimizing SSE, unless $\lambda = 0$, in which case we get back to the \textbf{ISMF}. The authors call this MF variant RISMF, that stands for regularized incremental simultaneous MF.

Similar to the ISMF approach, we compute the gradient of ${e_{ui}}'$:

\begin{eqnarray}
  \label{eq:gradient}
  \frac{\partial{e_{ui}}'}{\partial{\mathbf{p_{uk}}}} &=& -e_{ui} \cdot \mathbf{q_{ki}} + \lambda * p_{uk}, \nonumber \\
  \frac{\partial{e_{ui}}'}{\partial{\mathbf{q_{ki}}}} &=& -e_{ui} \cdot \mathbf{p_{uk}} + \lambda * p_{ki} 
\end{eqnarray}

We update the weights in the direction opposite to the gradient:

\begin{eqnarray}
  p_{uk} &\leftarrow& p_{uk} + \gamma \cdot ( e_{ui} \cdot q_{ki} - \lambda * p_{uk} ) \\
  q_{ki} &\leftarrow& q_{ki} + \gamma \cdot ( e_{ui} \cdot p_{uk} - \lambda * p_{ki} )
\end{eqnarray}

The training algorithm is for training the data can be found in \cite{takacs09scalable}:

\section{The Rho framework}
\label{sec:rho}

This section presents a small framework implemented by the author, that uses the algorithms mentioned in the previous chapter and which he called \emph{Rho}. It can be used to train a model, analyze the results and provide recommendations for a user. Starting with the overall architecture, in which the main components of the software are presented, we then show the parameters supported by the framework on each of the four components and how to use each component.

The purpose of \emph{Rho} is to provide a framework for recommender systems research, having a couple of tools for training, analyzing the results and making recommendations. It is formed of four components: \textbf{Trainer}, \textbf{BatchRunner}, \textbf{Analyzer}, \textbf{Recommender}. Next, we will analyze each component and explain what it does. 

\subsection{Training the model with the \textbf{Trainer}}
\label{trainer}

The main purpose of Rho is two allow making recommendations using matrix factorization techniques. Since it's unfeasible to factorize big sparse matrixes, the proposed algorithms uses some machine learning techniques. Discovering the model using machine learning assumes two phases: 1) train the model on a training dataset, 2) test the model on a test dataset, with the information gathered during training. As its name suggests, the Trainer component is used to train the model, on a training dataset. The script located in \texttt{trainer.py} runs the effective training algorithm. In that file, one has the ability to modify the parameters.

For now, Rho supports the 2 algorithms that were described earlier in sections \ref{ismf} and \ref{rismf} respectively. The parameters used to tune the algorithm are written in a Python hash format, which pretty easy to understand and follow. They are the following:
\emph{algorithm} (ISMF or RISMF), \emph{minimum improvement required to continue current feature}, \emph{learning rate}, \emph{regularization factor}, \emph{number of features(factors)} to use, \emph{initialization value for features}, \emph{max epochs per feature}, \emph{minimum number of epochs}, \emph{number of movies in entire training set}, \emph{number of users in entire training set}, \emph{number of ratings in entire training set}, \emph{path to training dataset}, \emph{path to test dataset}.

After running a training round, the results (the movies features and user features vectors corresponding to $P*$ and $Q*$ respectively) are stored within the results filed in the following format:\newline \texttt{Features\_[DAY]-[MONTH]-[YEAR]\_[HOUR]-[MINUTE].txt} where,\newline \texttt{[DAY], [MONTH], [YEAR], [HOUR], [MINUTE]} refer to the current date of the system. The data is serialized using the \texttt{cPickle} python library, and contains \texttt{Numpy} vectors (see section \ref{tehcnologies} for details). 

Further more, is the user choses the option to store additional results about that training round, things like RMSE, and the params with which the algorithm had rand, he can do that by enabling the \texttt{RECORD\_RESULTS\_TO\_SQL} option. This stores the results in a sqlite3 database (it's format is the same as table \ref{table:sqlite3_schema}). 

We can find the corresponding files for the user and movie feature vectors by having a look at the date field. 

\subsection{Analyzing the results with the \texttt{Analyzer}}

The Analyzer is a script which analyzes the results. We can use it in order to measure the efficiency of some of the tests we ran. This means, the script loads up the model, and tries to predict the ratings in the test file (with the prediction formula used on chapter 3). The level of acceptance, when analyzing weather a prediction was good or bad, can be adjusted using the \texttt{tolerance} parameter ($rating \in [predicted\_rating - tolerance, predicted\_rating + tolerance]$). For example, if the rating is $4.0$, the predicted rating is $3.7$, and the tolerance is $0.5$, the prediction is considered to be a success.  The parameters are also expressed in python hashes and can be configured within the script. 

If no training/test files are specified, the script will load all the results existing in the database described earlier, and test them. The tolerance would be the same and can be modified within the script. This is useful when trying to analyze all the experiments carried so far. 

\subsection{Carrying multiple experiments using the \texttt{BatchTrainer}}

We have found that running one experiment at a time can be a tedious and boring operation unless we really must do that. Most experiments usually involve changing some parameters and re-running the algorithm, whose speed can vary between a couple of seconds to tens of minutes. 

The \texttt{BatchTrainer} overcomes this problem by allowing us to describe experiments, and ultimately creating workflows for running multiple algorithms sequentially. 

We are able to state multiple parameters for different experiments in the \texttt{BatchTrainer.py} script (their meaning is the same as those described in section \ref{trainer}). The script will run the experiments one after the other, registering the results. 

\subsection{A recommendation service using the \texttt{Recommender}}

\texttt{Recommender.py} provides a service for querying for user preferences. Given a user id and based on a model, the user returns a list of 10 movies, that it thinks the user would rate as high. 

The params are the user feature file in the model learned and the user feature file in the model learned.

For example, when asking, ``\emph{What movies should I recommend to user 1 ?}'', the recommender would respond with a list of movie ids and the corresponding predicted rating.

\subsection{Technologies used}
\label{tehcnologies}

The implementation of \emph{Rho} is done using Python 2.6. The reason for using Python is that I wanted to model and test the different params quickly, rather then optimizing the algorithm for speed. For more about optimizing the algorithm see chapter 6, section Further improvments. For efficiently storing and working with the arrays and matrices, the \texttt{NumPy} library \cite{numpy} was used. The code for this framework can be found in \cite{rho}


\section{Results}

This chapter presents the results obtained by the author when running the implementation described in previous chapter. It starts by presenting the dataset (the Movie Lens dataset \cite{MOVIELENS-DATA}). Then it describes the attempt to get the correct $\gamma$ and $\lambda$ parameters, for minimizing the RMSE error on the models. We also analyzed how does the algorithms performs relative to the number of training epochs or features. Also we were interested in the time needed to run the experiments and the eventual correlation between it and the RMSE evolution. In the end of the chapter, we present a more comprehensive table with many values that have been obtained during the experiment.

\subsection{Datasets}
\label{datasets}

The experiments presented in this article have been carried out using the MovieLens database \cite{MOVIELENS-DATA}. MovieLens data sets were collected by the GroupLens Research Project at the University of Minnesota.
 
This data set consists of:
\begin{itemize}
	\item 100,000 ratings (1-5) from 943 users on 1682 movies. 
	\item Each user has rated at least 20 movies. 
\end{itemize}

Users and items are numbered consecutively from 1. The data is randomly ordered. This is a tab separated list of user \texttt{id | item id | rating | timestamp}.

Regarding the tests dataset: \texttt{u1.base} and \texttt{u1.test} through \texttt{u5.base} and \texttt{u5.test} are 80\%/20\% splits of the u data into training and test data. Each of \texttt{u1, ..., u5} have disjoint test sets; this if for 5 fold cross validation (where you repeat your experiment with each training and test set and average the results).

\subsection{Experiments and results}

It has been observed that RSIMF (with regularization factor usually performs better than ISMF). As described throughout this thesis, the idea is to minimize the RMSE error in order to obtain better results. 

First we wanted to see what are the best learning rates and regularization factors. For that we have tried a couple of tests with different values for the two parameters, which we presented in table \ref{tab:regularization}. In order to obtain a reasonable training we have used \textbf{20 features} and \textbf{50 epochs}. We noticed that not every feature was trained 50 times. If the improvement between two epochs is not grater than 0.0001, we move on to training the next feature. The smaller RMSE obtain was $0.7011$ which we have achieved for $\gamma = 0.02$ and $\lambda = 0.005$.

\begin{table}[tp]%
  \caption{Different RMSEs for $\lambda$ - regularization factor and $\gamma$ - learning rate}
  \centering%
  \begin{tabular}{cccccc}
  \toprule%
  $\lambda$  \textbackslash $\gamma$ & 0.005 & 0.007 & 0.01 & 0.015 & 0.02 \\ \midrule
  0.005 & 0.9333 & 0.8815 & 0.8462 & 0.7097 & \textbf{0.7011} \\
  0.007 & 0.9333 & 0.8815 & 0.8463 & 0.7168 & 0.7026 \\
  0.01  & 0.9333 & 0.8816 & 0.8465 & 0.7197 & 0.7027 \\
  0.015 & 0.9333 & 0.8819 & 0.8470 & 0.7263 & 0.7057 \\ \bottomrule
  \end{tabular}
  \label{tab:regularization}
\end{table}

\begin{table}
  \centering
  \begin{tabular}{cc}
      \centering
        \includegraphics[width=0.5\textwidth]{../thesis/charts/running_times_for_20_trainings}\label{fig:time}
    &
    \centering
      \includegraphics[width=0.5\textwidth]{../thesis/charts/total_rmse_evolution}\label{fig:rmse_total}
  \end{tabular}
\end{table}

The running time for these experiments was about 70962 seconds, about 19.71 hours. The medium training is 0.98 hours. We show this on figure \ref{fig:time}. We notice that the time to train the models which yielded best RMSE is significantly longer than that used to train models with lower RMSE.

% Figure \ref{fig:rmse_evolution} shows RMSE evolution training on one epoch 20 features, versus 50 epochs 20 features. It is clearly that the more epochs are involved, the better the predictions are. 

We have also noticed in figure \ref{fig:rmse_total} a certain periodicity on the rmse. The values plotted here are from the same experiment presented in table \ref{tab:regularization} and follows the same rule as figure \ref{fig:time}.

% \begin{figure}[h!]
%   \caption{RMSE evolution between two epochs}
%   \centering
%     \includegraphics[width=0.7\textwidth]{../thesis/charts/rmse_evolution}\label{fig:rmse_evolution}
% \end{figure}

Table \ref{tag:final_results} offers a complete overview over the experiments which have been ran. 

% \begin{landscape}
  \begin{table}[ht]%
    \caption{Table format for storing training rounds results}
    \label{table:sqlite3_schema}\centering%
    \small\addtolength{\tabcolsep}{-1pt}
    \begin{tabular}{ccccccccc}
    \toprule%
    alg & epochs & features & time(sec) & RMSE & training DS   & test DS    \\ \toprule
    RISMF & 50 & 20 & 826 &  0.9332 & dataset/u1.base & dataset/u1.test \\
    RISMF & 50 & 20 & 1695 & 0.8815 & dataset/u1.base & dataset/u1.test\\
    RISMF & 50 & 20 & 2354 & 0.8462 & dataset/u1.base & dataset/u1.test \\
    RISMF & 50 & 20 & 6795 & 0.7097 & dataset/u1.base & dataset/u1.test\\
    RISMF & 50 & 20 & 6097 & 0.7011 & dataset/u1.base & dataset/u1.test\\
    RISMF & 50 & 20 & 817 &  0.9332 & dataset/u1.base & dataset/u1.test \\
    RISMF & 50 & 20 & 1695 & 0.8815 & dataset/u1.base & dataset/u1.test\\
    RISMF & 50 & 20 & 2361 & 0.8463 & dataset/u1.base & dataset/u1.test\\
    RISMF & 50 & 20 & 6522 & 0.7168 & dataset/u1.base & dataset/u1.test\\
    RISMF & 50 & 20 & 6162 & 0.7026 & dataset/u1.base & dataset/u1.test\\
    RISMF & 50 & 20 & 813 &  0.9333 & dataset/u1.base & dataset/u1.test \\
    RISMF & 50 & 20 & 2801 & 0.8816 & dataset/u1.base & dataset/u1.test\\
    RISMF & 50 & 20 & 2357 & 0.8465 & dataset/u1.base & dataset/u1.test\\
    RISMF & 50 & 20 & 6915 & 0.7107 & dataset/u1.base & dataset/u1.test\\
    RISMF & 50 & 20 & 6129 & 0.7027 & dataset/u1.base & dataset/u1.test\\
    RISMF & 50 & 20 & 817 &  0.9334 & dataset/u1.base & dataset/u1.test \\
    RISMF & 50 & 20 & 1685 & 0.8819 & dataset/u1.base & dataset/u1.test\\
    RISMF & 50 & 20 & 2379 & 0.8470 & dataset/u1.base & dataset/u1.test\\
    RISMF & 50 & 20 & 6307 & 0.7263 & dataset/u1.base & dataset/u1.test\\
    RISMF & 50 & 20 & 6261 & 0.7057 & dataset/u1.base & dataset/u1.test\\  \bottomrule
    \end{tabular}
    \label{tag:final_results}
  \end{table}

\newpage

\bibliographystyle{unsrtnat}
\bibliography{article}


\end{document}  
